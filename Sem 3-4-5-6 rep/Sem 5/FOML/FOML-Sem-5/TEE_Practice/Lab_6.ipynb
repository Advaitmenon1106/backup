{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01af1859",
   "metadata": {},
   "source": [
    "# Creating a simple NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9e804af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.62434536 -0.61175641 -0.52817175]\n",
      " [-1.07296862  0.86540763 -2.3015387 ]]\n"
     ]
    }
   ],
   "source": [
    "##Activation Function\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "X=np.random.randn(2,3)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff54795",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The sigmoid function takes any real number as input and transforms it into a value between 0 and 1.\n",
    "#The output of the sigmoid function, often denoted as σ(z) or simply s, represents the probability\n",
    "#that a given input belongs to the positive class in a binary classification problem. When z is \n",
    "#positive and large, σ(z) approaches 1. When z is negative and large, σ(z) approaches 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca794ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.Sigmoid Function\n",
    "def sigmoid(z):\n",
    "        s=1/(1+np.exp(-z))\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b97a2460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.83539354 0.35165864 0.3709434 ]\n",
      " [0.25483894 0.70378922 0.09099561]]\n"
     ]
    }
   ],
   "source": [
    "s=sigmoid(X)  #sigmoid function will be applied element-wise to each element of the array.\n",
    "print(s)     #contains the sigmoid values computed from the elements of matrix X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998a834c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The hyperbolic tangent function, often denoted as tanh⁡(z), is another activation function that\n",
    "#maps input values to the range between -1 and 1.\n",
    "#useful when the data distribution has both positive and negative values and centerd around zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6b98c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. tanh Function\n",
    "def tan(z):    \n",
    "    s = (np.exp(z)-np.exp(-z))/(np.exp(z)+np.exp(-z))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2e389a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.92525207 -0.5453623  -0.48398233]\n",
      " [-0.79057703  0.69903334 -0.98015695]]\n"
     ]
    }
   ],
   "source": [
    "t=tan(X)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10e8ac34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. relu Function\n",
    "def relu(z):\n",
    "    s = np.maximum(0,z)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d7a529",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if the input value z is positive, the output value s will be equal to z; otherwise, it will be \n",
    "#set to 0. The np.maximum function from the NumPy library is used to element-wise compare each value\n",
    "#in the input array z with 0 and take the maximum.\n",
    "\n",
    "#It's known for its simplicity and effectiveness in addressing the vanishing gradient problem. When \n",
    "#the input is positive, ReLU maintains the gradient as 1, which prevents the gradients from vanishing \n",
    "#during backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b13afa90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.62434536 0.         0.        ]\n",
      " [0.         0.86540763 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "r=relu(X)\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ea02b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Leaky relu:-\n",
    "\n",
    "#if the input value z is positive, the output value s will be equal to z; if z is negative, the \n",
    "#output value s will be 0.01z. The np.maximum function from the NumPy library is used to perform\n",
    "#the element-wise comparison and take the maximum value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013542ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It aims to address the issue of \"dying ReLUs,\" where neurons can become inactive during training by \n",
    "#setting the output to 0 for all inputs. The leaky parameter (0.01 in this case) introduces a small \n",
    "#negative slope for the negative values of z, allowing some gradient to flow even for negative inputs.\n",
    "#This helps prevent neurons from becoming completely inactive and can help alleviate vanishing \n",
    "#gradient issues to some extent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3265eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. leaky relu Function\n",
    "def lrelu(z):\n",
    "    s = np.maximum(0.01*z,z)   #0.01 is the \"leakiness\" parameter\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e7bd5333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.62434536 0.01       0.01      ]\n",
      " [0.01       0.86540763 0.01      ]]\n"
     ]
    }
   ],
   "source": [
    "ler=lrelu(X)\n",
    "print(ler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fd24c0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Softmax Function\n",
    "def softmax(vector):          \n",
    "    e = np.exp(vector)    #compute the exponential function element-wise.\n",
    "    s= e / e.sum()     #computes the softmax values by dividing each exponential value by the sum of all exponential values.\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5215eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#s is an array where each element represents the probability of the corresponding element in the input\n",
    "#vector.\n",
    "#commonly used in classification tasks, especially in multi-class classification scenarios. It takes \n",
    "#a vector of raw scores (logits) and transforms them into a probability distribution over multiple \n",
    "#classes. The softmax function ensures that the output values are between 0 and 1 and that they sum \n",
    "#up to 1, making them suitable as probabilities. The class with the highest probability becomes the \n",
    "#predicted class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "13dad5a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.56232214 0.06009779 0.06533695]\n",
      " [0.03789279 0.26325869 0.01109163]]\n"
     ]
    }
   ],
   "source": [
    "sof=softmax(X)\n",
    "print(sof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f463cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The derivative helps propagate gradients backward through the network to adjust the weights and \n",
    "#biases during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d0aa62fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "##derivatives of Activation Function\n",
    "#1.Derivatives of Sigmoid Function\n",
    "def dsig(s):\n",
    "    das = (s)*(1-s)\n",
    "    return das"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca4c1180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.13751118 0.22799484 0.23334439]\n",
      " [0.18989606 0.20846995 0.08271541]]\n"
     ]
    }
   ],
   "source": [
    "ds=dsig(s)\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cf4034",
   "metadata": {},
   "outputs": [],
   "source": [
    "#derivative of the tanh function is used in backpropagation during the training of neural networks.\n",
    "#It helps propagate gradients backward through the network to adjust the weights and biases during \n",
    "#training.\n",
    "#In some contexts, the tanh function and its derivative can be advantageous over the sigmoid function\n",
    "#because the tanh function is zero-centered and can produce both positive and negative outputs. This \n",
    "#can lead to more balanced gradients during training and potentially faster convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a077cc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.Derivatives of tanh Function\n",
    "def dthan(s):\n",
    "    dat = (1-s**2)\n",
    "    return dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "764bb7a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.92525207 -0.5453623  -0.48398233]\n",
      " [-0.79057703  0.69903334 -0.98015695]]\n"
     ]
    }
   ],
   "source": [
    "dt=dthan(t)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a1997e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The derivative of ReLU is 1 for positive inputs and 0 for non-positive inputs. This line uses a \n",
    "#comparison operation (s > 0) to create a boolean array where each element indicates whether the \n",
    "#corresponding element in the input s is greater than 0. The np.int64 function is used to convert \n",
    "#the boolean array into an integer array where True becomes 1 and False becomes 0.\n",
    "\n",
    "#when the input is non-positive, the derivative is 0, causing the gradient to vanish. This behavior\n",
    "#can lead to \"dead\" neurons that don't update during training, which is a challenge associated with \n",
    "#ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "386f2f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.Derivatives of Relu Function\n",
    "def drelu(s):\n",
    "    dar=(np.int64(s>0))\n",
    "    return dar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "33f8aa30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0]\n",
      " [0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "dr = drelu(r)\n",
    "print(dr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ebf24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dlrelu that takes two arguments: s, which is the output of the Leaky ReLU function, and alpha, \n",
    "#which is a parameter that determines the slope of the function for negative inputs. By default, \n",
    "#alpha is set to 0.01.\n",
    "\n",
    "# Derivatives of leaky Relu is 1 for positive inputs and alpha for non-positive inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ab0e2245",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.Derivatives of leaky Relu Function\n",
    "#The np.where function is used to create an array where each element is 1 if the corresponding \n",
    "#element in the input s is greater than 0, and alpha if the element is non-positive.\n",
    "\n",
    "def dlrelu(s,alpha=0.01):\n",
    "    dal=np.where(s >0, 1, alpha)\n",
    "    return dal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1cf8105b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1.],\n",
       "       [1., 1., 1.]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dler=dlrelu(ler,0.01)\n",
    "print(dler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720be21d",
   "metadata": {},
   "source": [
    "Basic NN Model (layers-2,4 Nodes in each Layers,1 out put layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a1846578",
   "metadata": {},
   "outputs": [],
   "source": [
    "##1.Input X and Y\n",
    "np.random.seed(1)\n",
    "X=np.random.randn(2,3)\n",
    "Y=(np.random.randn(1,3)>0) #generates a 1x3 matrix of random numbers, converts these values into a\n",
    "#boolean matrix by checking if each value is greater than 0\n",
    "#    Y is a NumPy array with shape (1, 3), where:\n",
    "        #The first dimension (rows) corresponds to the number of classes (binary in this case).\n",
    "        #The second dimension (columns) corresponds to the number of samples.\n",
    "#In a binary classification task, such as this one, you typically have only one class label for \n",
    "#each sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "87b37c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.62434536 -0.61175641 -0.52817175]\n",
      " [-1.07296862  0.86540763 -2.3015387 ]]\n",
      "\n",
      "(2, 3)\n",
      "\n",
      "[[ True False  True]]\n"
     ]
    }
   ],
   "source": [
    "print(X)  #nx=2 and m=3\n",
    "print()\n",
    "print(X.shape)\n",
    "print()\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "36d2d861",
   "metadata": {},
   "outputs": [],
   "source": [
    "##2.Define shape\n",
    "#three arguments: X, which represents the input data, Y, which represents the labels, and layers, \n",
    "#which is the number of units in the hidden layer.\n",
    "\n",
    "def layer_sizes(X, Y,layers):\n",
    "    n_x=X.shape[0]\n",
    "    n_h=layers\n",
    "    n_y=Y.shape[0]\n",
    "    return (n_x,n_h,n_y)\n",
    "\n",
    "#returns a tuple containing the calculated values of n_x, n_h, and n_y, which correspond to the \n",
    "#number of features, number of units in the hidden layer, and number of classes (labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3e3172e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_x,n_h,n_y = layer_sizes(X, Y, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "69cbd943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 4 1\n"
     ]
    }
   ],
   "source": [
    "print(n_x,n_h,n_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1c3c32ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "##3. Initialize the parameters\n",
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    np.random.seed(2) # we set up a seed so that your output matches ours although the initialization is random.\n",
    "    \n",
    "    W1 = np.random.randn(n_h,n_x)*0.01  ##n_h=4,n_x=2\n",
    "    print(W1.shape)\n",
    "    b1 = np.zeros((n_h,1))          ##n_h=4\n",
    "    W2 = np.random.randn(n_y,n_h)*0.01   ##n_y=1,n_h=4\n",
    "    b2 = np.zeros((n_y,1))            ##n_y=1\n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "   \n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "df2e4e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 2)\n",
      "{'W1': array([[-0.00416758, -0.00056267],\n",
      "       [-0.02136196,  0.01640271],\n",
      "       [-0.01793436, -0.00841747],\n",
      "       [ 0.00502881, -0.01245288]]), 'b1': array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]]), 'W2': array([[-0.01057952, -0.00909008,  0.00551454,  0.02292208]]), 'b2': array([[0.]])}\n"
     ]
    }
   ],
   "source": [
    "parameters=initialize_parameters(n_x, n_h, n_y)\n",
    "print(parameters) \n",
    "\n",
    "#initialized weight matrices and bias vectors for both layers of the neural network.\n",
    "#These parameters will be used as the initial starting point for training the neural network using \n",
    "#optimization algorithms like gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b4d7138d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##4. Forward Propagation\n",
    "#two arguments: X, which represents the input data, and parameters, which is a dictionary containing \n",
    "#the weight matrices and bias vectors for both layers of the neural network.\n",
    "\n",
    "#extract the weight matrices and bias vectors for both layers from the parameters dictionary.\n",
    "def forward_propagation(X, parameters):\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "\n",
    "    Z1 = np.dot(W1,X)+b1 #Z1 is the linear transformation of the input X using the weights W1 and bias b1\n",
    "    A1 = np.tanh(Z1)\n",
    "    Z2 = np.dot(W2,A1)+b2 #linear transformation of the hidden layer activation A1 using the weights W2 and bias b2\n",
    "    A2 = sigmoid(Z2)\n",
    "\n",
    "#store intermediate values computed during forward propagation.\n",
    "    cache = {\"Z1\": Z1,\n",
    "             \"A1\": A1,\n",
    "             \"Z2\": Z2,\n",
    "             \"A2\": A2}\n",
    "    \n",
    "    return A2, cache\n",
    "#returns the final output A2 (the predicted values) and intermediate value\n",
    "#These intermediate values are essential for backpropagation during the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "caeedd09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5002307  0.49985831 0.50023963]] {'Z1': array([[-0.00616586,  0.00206261,  0.0034962 ],\n",
      "       [-0.05229879,  0.02726335, -0.02646868],\n",
      "       [-0.0200999 ,  0.00368691,  0.02884556],\n",
      "       [ 0.02153008, -0.01385323,  0.02600471]]), 'A1': array([[-0.00616578,  0.0020626 ,  0.00349619],\n",
      "       [-0.05225116,  0.02725659, -0.0264625 ],\n",
      "       [-0.0200972 ,  0.00368689,  0.02883756],\n",
      "       [ 0.02152676, -0.01385234,  0.02599885]]), 'Z2': array([[ 0.00092281, -0.00056678,  0.00095853]]), 'A2': array([[0.5002307 , 0.49985831, 0.50023963]])}\n"
     ]
    }
   ],
   "source": [
    "A2, cache=forward_propagation(X, parameters)\n",
    "print(A2, cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ec7dc08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##5.Compute Cost\n",
    "#calculates the cost (loss) of the neural network's predictions compared to the actual labels.\n",
    "def compute_cost(A2, Y, parameters):\n",
    "    m=Y.shape[1]     #training examples\n",
    "    logprobs = np.multiply(np.log(A2),Y)+np.multiply(np.log(1-A2),(1-Y))\n",
    "    cost = -np.sum(logprobs)/m\n",
    "    cost = float(np.squeeze(cost)) #converts the cost value to a scalar by using the np.squeeze function to remove any unnecessary dimensions. \n",
    "    return cost\n",
    "\n",
    "#This cost value represents how well the network's predictions match the true labels, and it is \n",
    "#used as a measure of how well the network is performing during training.\n",
    "\n",
    "#negative sign in the NLL formula makes it convenient for optimization algorithms. Most optimization \n",
    "#algorithms are designed to minimize functions, so the negative sign transforms the problem into a \n",
    "#minimization task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c674d8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cost=−m1​∑i=1m​(yi​log(y^​i​)+(1−yi​)log(1−y^​i​))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "71c8e6a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6927392477233995"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_cost(A2, Y, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "11600115",
   "metadata": {},
   "outputs": [],
   "source": [
    "##6. Backward Propagation\n",
    "#extract the weight matrices and bias vectors\n",
    "#extract the activation values A1 and A2 from the cache dictionary\n",
    "\n",
    "def BWP(parameters,cache,X,Y):\n",
    "    W1=parameters[\"W1\"]\n",
    "    b1=parameters[\"b1\"]\n",
    "    W2=parameters[\"W2\"]\n",
    "    b2=parameters[\"b2\"]\n",
    "    A1=cache[\"A1\"]\n",
    "    A2=cache[\"A2\"]\n",
    "\n",
    "    m=Y.size   #number of samples in the dataset by accessing the total number of elements in the Y array.\n",
    "\n",
    "#derivative of the cost with respect to Z2, which is the difference between the predicted output A2 \n",
    "#and the actual labels Y\n",
    "    dZ2=A2-Y\n",
    "    dW2=np.dot(dZ2,A1.T)/m  #gradient of the weights\n",
    "    db2=np.sum(dZ2,axis=1,keepdims=True)/m    #gradient of the bias\n",
    "    dA1=np.dot(W2.T,dZ2)\n",
    "    dZ1=dA1*(1-np.power(A1,2))\n",
    "    dW1=np.dot(dZ1,X.T)/m\n",
    "    db1=np.sum(dZ1,axis=1,keepdims=True)/m\n",
    "\n",
    "    grades={\"dW2\":dW2,\"db2\":db2,\"dW1\":dW1,\"db1\":db1}\n",
    "    #contains the calculated gradients for the weights and biases of both layers.\n",
    "    return grades\n",
    "\n",
    "#gradients are crucial for updating the parameters during the optimization process, such as gradient \n",
    "#descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "300802ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dW2': array([[ 0.00078841,  0.01765429, -0.00084166, -0.01022527]]), 'db2': array([[-0.16655712]]), 'dW1': array([[ 0.00301023, -0.00747267],\n",
      "       [ 0.00257968, -0.00641288],\n",
      "       [-0.00156892,  0.003893  ],\n",
      "       [-0.00652037,  0.01618243]]), 'db1': array([[ 0.00176201],\n",
      "       [ 0.00150995],\n",
      "       [-0.00091736],\n",
      "       [-0.00381422]])}\n"
     ]
    }
   ],
   "source": [
    "grades=BWP(parameters,cache,X,Y)\n",
    "print(grades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c165b0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##7.Update Grades\n",
    "def update(parameters,grades,lr=0.01):\n",
    "    W1=parameters[\"W1\"]\n",
    "    b1=parameters[\"b1\"]\n",
    "    W2=parameters[\"W2\"]\n",
    "    b2=parameters[\"b2\"]\n",
    "\n",
    "    dW1=grades[\"dW1\"]\n",
    "    db1=grades[\"db1\"]\n",
    "    dW2=grades[\"dW2\"]\n",
    "    db2=grades[\"db2\"]\n",
    "\n",
    "\n",
    "    W1=W1-lr*dW1  #core step of the optimization process, here the parameters are adjusted to minimize the cost function.\n",
    "    b1=b1-lr*db1\n",
    "    W2=W2-lr*dW2\n",
    "    b2=b2-lr*db2\n",
    "\n",
    "    parameters={\"W1\":W1,\"b1\":b1,\"W2\":W2,\"b2\":b2}\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "82b9539d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'W1': array([[-0.00419768, -0.00048794],\n",
      "       [-0.02138776,  0.01646684],\n",
      "       [-0.01791867, -0.0084564 ],\n",
      "       [ 0.00509402, -0.01261471]]), 'b1': array([[-1.76201370e-05],\n",
      "       [-1.50994736e-05],\n",
      "       [ 9.17363463e-06],\n",
      "       [ 3.81421789e-05]]), 'W2': array([[-0.01058741, -0.00926662,  0.00552296,  0.02302433]]), 'b2': array([[0.00166557]])}\n"
     ]
    }
   ],
   "source": [
    "parameters=update(parameters,grades,lr=0.01)\n",
    "print(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1b22e38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##8.creat own NN\n",
    "def NN(X,Y,layers,itr=10000,print_cost=False):\n",
    "    np.random.seed(3)\n",
    "    n_x=layer_sizes(X, Y,layers)[0]\n",
    "    n_y=layer_sizes(X, Y,layers)[2]\n",
    "    n_h=layer_sizes(X, Y,layers)[1]\n",
    "\n",
    "    parameters=initialize_parameters(n_x, n_h, n_y)\n",
    "\n",
    "    for i in range (0,itr):\n",
    "        A2, cache = forward_propagation(X, parameters)\n",
    "        cost=compute_cost(A2, Y, parameters)\n",
    "        grades=BWP(parameters,cache,X,Y)\n",
    "        parameters=update(parameters,grades,lr=0.01)\n",
    "\n",
    "        if print_cost and i%100==0:\n",
    "            print(\"cost % i:%f\" %(i,cost))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c6160e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost  0:0.692739\n",
      "cost  100:0.669215\n",
      "cost  200:0.649677\n",
      "cost  300:0.619729\n",
      "cost  400:0.555289\n",
      "cost  500:0.447120\n",
      "cost  600:0.328614\n",
      "cost  700:0.234149\n",
      "cost  800:0.169726\n",
      "cost  900:0.127562\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'W1': array([[-0.21830665,  0.43559748],\n",
       "        [-0.38633057,  0.73732683],\n",
       "        [ 0.11624791, -0.28270074],\n",
       "        [ 0.39639168, -0.77368548]]),\n",
       " 'b1': array([[ 0.03393657],\n",
       "        [ 0.09586648],\n",
       "        [-0.01517238],\n",
       "        [-0.10360464]]),\n",
       " 'W2': array([[-0.52988031, -1.04262947,  0.31610115,  1.11019047]]),\n",
       " 'b2': array([[0.34409584]])}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN(X,Y,4,itr=1000,print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db2a94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The vanishing gradient problem is a challenge that arises during the training of deep neural networks,\n",
    "#particularly when using gradient-based optimization algorithms like backpropagation. It occurs when \n",
    "#the gradients of the loss function with respect to the network's parameters (weights and biases) \n",
    "#become extremely small as they are propagated backward through the layers of the network. As a result,\n",
    "#the weights of the earlier layers receive very small updates during training, leading to slow \n",
    "#convergence or even stagnation in learning.\n",
    "#Sigmoid and tanh activation functions are susceptible to the vanishing gradient problem because:\n",
    "\n",
    "#In the sigmoid function, the gradient approaches zero as the input becomes very large or very small.\n",
    " #In the tanh function, the gradient also becomes small for large positive and negative inputs.\n",
    "\n",
    "#This can result in the early layers of the network learning very slowly or not learning at all. \n",
    "#As a consequence, the network's ability to capture complex patterns in the data decreases, limiting \n",
    "#its overall performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642c21e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
